# Describe your process for conducting code reviews and ensuring best practices.

## Why This Is Asked

Interviewers want to see that you treat code review as a quality and learning mechanism, not a bottleneck. They're looking for clear expectations, constructive feedback, and a balance between thoroughness and speed.

## Key Points to Cover

- What you look for in reviews (correctness, design, readability, tests)
- How you give feedback (constructive, specific, kind)
- Expectations for turnaround time and reviewer responsibility
- How you use reviews for knowledge sharing and standards

## STAR Method Answer Template

### Situation
_[Describe the context - what was happening, what team/company, what was at stake]_

### Task
_[What was your specific responsibility or challenge?]_

### Action
_[What specific steps did you take? Be detailed about YOUR actions]_

### Result
_[What was the outcome? Use metrics where possible. What did you learn?]_

## Tips

- Reference checklists or PR templates if you use them
- Mention how you've improved the review process over time

## Example Response

**Situation:** At a fintech startup, code reviews had become a bottleneck—PRs sat for 3–5 days, and when feedback came, it was often subjective ("I'd do it differently") rather than actionable. Junior engineers felt discouraged; senior engineers felt overwhelmed. We needed to fix the process without sacrificing quality.

**Task:** I needed to establish clear expectations, reduce cycle time, and make reviews a learning mechanism rather than a gate.

**Action:** I introduced a PR template with checkboxes: tests added, docs updated, no new lint errors, and a brief description of the change. I set a 24-hour SLA for first review and made it visible in our standups. I created a lightweight review guide: focus on correctness, security, and maintainability; avoid style nitpicks (we automated those with linting). I also ran a session on giving constructive feedback—using "consider X" instead of "this is wrong." I paired junior engineers with seniors for their first few PRs so they learned the expectations. I tracked PR cycle time and shared it in retros.

**Result:** PR cycle time dropped from 4 days to 1.2 days. We saw fewer post-merge bugs. Junior engineers reported feeling more confident. I learned that clear expectations and SLAs reduce friction—and that automating style decisions frees reviewers to focus on what matters.

## Companies Known to Ask This

| Company | Variation / Focus |
|---------|------------------|
| Amazon | Insist on Highest Standards — "How do you run code reviews?" |
| Google | Technical excellence, collaboration |
| Meta | High-performing teams, quality |
| Microsoft | Quality, execution |
| Stripe | Building reliable systems |
| Apple | Craft and quality |
