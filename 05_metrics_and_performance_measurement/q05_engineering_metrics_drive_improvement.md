# How do you use engineering metrics (e.g., DORA metrics) to drive improvement?

## Why This Is Asked

Interviewers want to see that you use data to guide decisions—not just collect it. They're looking for evidence that you connect metrics to actionable changes, involve the team in improvement efforts, and avoid using metrics punitively.

## Key Points to Cover

- Choosing metrics that matter (deployment frequency, lead time, change failure rate, MTTR)
- Sharing metrics transparently with the team and using them for retrospectives
- Running experiments to improve (e.g., reducing deployment friction)
- Tying metrics to outcomes, not just activity

## STAR Method Answer Template

### Situation
_[Describe the context - what was happening, what team/company, what was at stake]_

### Task
_[What was your specific responsibility or challenge?]_

### Action
_[What specific steps did you take? Be detailed about YOUR actions]_

### Result
_[What was the outcome? Use metrics where possible. What did you learn?]_

## Tips

- Name specific metrics (DORA, cycle time, etc.) and how you acted on them
- Emphasize improvement culture—metrics inform, not judge

## Example Response

**Situation:** I managed a team of 15 at a SaaS company. Our deployment frequency was once every two weeks, lead time was 12 days, and change failure rate was 18%. The team felt slow and frustrated. Leadership wanted to move faster.

**Task:** I was responsible for using engineering metrics to drive improvement without creating a blame culture.

**Action:** I introduced DORA metrics as a team dashboard—visible to everyone, not just leadership. I framed them as improvement signals, not performance evaluation. In our retrospectives, we picked one metric per quarter to focus on. First, we tackled deployment frequency: we invested in CI/CD improvements, parallelized tests, and introduced feature flags. I ran experiments—A/B testing smaller PRs, measuring impact. We went from 2 deploys/month to 15. Then we focused on change failure rate: we added automated rollback, improved test coverage, and ran pre-deployment checks. I shared wins in all-hands and credited the team.

**Result:** Within a year, deployment frequency went from 0.5/week to 3/week, lead time from 12 days to 2 days, and change failure rate from 18% to 4%. I learned that metrics work when the team owns them—they need to see the data, understand the "why," and drive the improvements themselves.

## Companies Known to Ask This

| Company | Variation / Focus |
|---------|------------------|
| Amazon | Invent & Simplify, Bias for Action — "How do you use data to improve?" |
| Google | Data-driven decisions, technical excellence |
| Meta | Moving fast, scale, impact |
| Microsoft | Execution, customer focus |
| Stripe | Technical judgment, moving fast in ambiguity |
| Uber | Entrepreneurship, building for scale |
