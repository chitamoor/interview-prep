# How do you distinguish between leading and lagging indicators for team health?

## Why This Is Asked

Interviewers want to see that you understand the difference between predictive signals (leading) and outcome measures (lagging). They're looking for evidence that you use both—leading indicators to intervene early, lagging indicators to validate impact—and that you don't over-rely on lagging metrics that only tell you what already happened.

## Key Points to Cover

- Defining leading indicators (e.g., PR review time, backlog health, morale signals)
- Defining lagging indicators (e.g., delivery rate, incident count, attrition)
- Using leading indicators for early intervention and course correction
- Using lagging indicators to validate that interventions worked

## STAR Method Answer Template

### Situation
_[Describe the context - what was happening, what team/company, what was at stake]_

### Task
_[What was your specific responsibility or challenge?]_

### Action
_[What specific steps did you take? Be detailed about YOUR actions]_

### Result
_[What was the outcome? Use metrics where possible. What did you learn?]_

## Tips

- Give concrete examples of leading vs. lagging indicators you use
- Show you act on leading indicators before problems show up in lagging data

## Example Response

**Situation:** I led a platform team at a high-growth startup. We kept getting surprised by delivery slips and incidents—by the time we saw problems in our lagging metrics (sprint completion rate, incident count), it was too late to prevent them.

**Task:** I needed to identify leading indicators that would let us intervene early.

**Action:** I mapped our lagging indicators (delivery rate, incident count, attrition) to potential leading signals. For delivery: PR review time, backlog health, and scope creep frequency. For incidents: test coverage trends, deployment frequency (more frequent = smaller changes = fewer failures), and on-call fatigue. For attrition: eNPS, 1:1 themes, and vacation usage. I built a weekly "leading indicators" dashboard and set thresholds—e.g., when PR review time exceeded 24 hours, we investigated. When backlog health dropped (too many stale items), we ran a grooming session. I trained the team to act on these signals: if we saw review time creeping up, we'd pause new work and clear the queue before it impacted delivery.

**Result:** We reduced surprise delivery slips by 60% and caught two engineers at burnout risk before they left. I learned that lagging indicators tell you what happened; leading indicators let you prevent it. Acting early is always cheaper than reacting late.

## Companies Known to Ask This

| Company | Variation / Focus |
|---------|------------------|
| Amazon | Dive Deep, Are Right a Lot — "How do you predict and prevent problems?" |
| Google | Navigating ambiguity, data-driven decisions |
| Meta | Scale, impact, moving fast with foresight |
| Microsoft | Execution under pressure, growth mindset |
| Stripe | Technical judgment, moving fast in ambiguity |
| Uber | Ownership, building for scale |
